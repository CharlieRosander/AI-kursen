{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Character classifier\n",
    "###### Charlie Rosander, 2023-08\n",
    "\n",
    "In this project we will create a CNN to recognise and classify english, handwritten characters (0-9, A-Z).\n",
    "\n",
    "The dataset we are using is the \"English handwritten characters\" from kaggle: \n",
    "https://www.kaggle.com/datasets/dhruvildave/english-handwritten-characters-dataset\n",
    "\n",
    "\"This dataset contains 3,410 images of handwritten characters in English. This is a classification dataset that can be used for Computer Vision tasks. It contains 62 classes with 55 images of each class. The 62 classes are 0-9, A-Z and a-z.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "Looking over the dataset i can see right away that all of the pictures are written in a program like paint and not by hand with pen and paper, which is a bit of a shame as this will most likely overfit the model and make the model perform worse on real handwritten characters.\n",
    "\n",
    "We will see how it performs and later on I will do a \"real-world\" test with my own handwriting to see if I can test the limits of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure\n",
    "\n",
    "We will start with the imports and then load the data into a dataframe. We will then do some preprocessing and then create the model, followed by the testing and analysis of the model. I will make the model savable so that we can continue to train it later on if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data and labeling it through indexing the filenames.\n",
    "This way I dont have to seperate the images into their own directories or rename the files etc, as looking over the data I realised right away that the first 3 digits in the filename represents that characters class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(directory_path, filename))\n",
    "            img = cv2.resize(img, (84, 84)) # or any size you want\n",
    "            img = img / 255.0 # Normalization\n",
    "            \n",
    "            # Extract label from filename\n",
    "            label = int(filename[3:6]) # Index 3-6 of the filename is the label\n",
    "            labels.append(label)\n",
    "            \n",
    "            images.append(img)\n",
    "        \n",
    "    images = np.array(images)\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "directory_path = \"./EngChars/Img/\"\n",
    "images, labels = load_images(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2728, 84, 84, 3)\n",
      "(682, 84, 84, 3)\n"
     ]
    }
   ],
   "source": [
    "# Kollar så att det ser ut som vi förväntar oss, vilket det gör\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "Here we create our model, I have chosen to use 4 hidden layers with 2 convolutional layers and 2 dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(84, 84, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
